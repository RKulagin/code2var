{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ruslan/Documents/course-project-TiMP\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, OrderedDict\n",
    "from sortedcontainers import SortedDict\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "from  matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаём гистограмму для колчества функций встречающихся в датасете в зависимости от их частоты.\n",
    "Огромная функция для построения графика зависимости количества элементов с данной частотой вхождений от этой частоты.\n",
    "\n",
    "Происходит парсинг вокабов, полученных при препроцессинге, создание гистограммы, а также вычисление размеров отфильтрованных данных для последующего вывода на график.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historgram(train_vocab, train_name, test_vocab, test_name, val_vocab, val_name, limit):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    train_vocab = {key:int(value) for key, value in train_vocab.items()}\n",
    "    train = SortedDict(sorted(Counter(train_vocab.values()).items()))\n",
    "    test_vocab = {key:int(value) for key, value in test_vocab.items()}\n",
    "    test = SortedDict(sorted(Counter(test_vocab.values()).items()))\n",
    "    val_vocab = {key:int(value) for key, value in val_vocab.items()}\n",
    "    val = SortedDict(sorted(Counter(val_vocab.values()).items()))\n",
    "    combined_keys = {*train.keys(), *test.keys(), *val.keys()}\n",
    "    train_missing_keys = combined_keys - train.keys()\n",
    "    train.update({key:0 for key in train_missing_keys})\n",
    "    test_missing_keys = combined_keys - test.keys()\n",
    "    test.update({key:0 for key in test_missing_keys})\n",
    "    val_missing_keys = combined_keys - val.keys()\n",
    "    val.update({key:0 for key in val_missing_keys})\n",
    "    \n",
    "    bad = {key:train[key]+test[key]+val[key] for key in train.keys() if key < limit}\n",
    "    plt.bar(list(train.keys()), list(train.values()), label=train_name)\n",
    "    plt.bar(list(test.keys()), list(test.values()), bottom=list(train.values()), color='green', label=test_name)\n",
    "    plt.bar(list(val.keys()), list(val.values()), bottom=np.sum((list(train.values()), list(test.values())), axis=0), color='red', label=val_name)\n",
    "    plt.bar(list(bad.keys()), list(bad.values()), color=\"black\", label=\"filtered\", alpha=0.4)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.xlim(0, 100)\n",
    "    plt.xlabel(\"Частота вхождения элемента в AST.\")\n",
    "    plt.ylabel(\"Количество элементов с такой частотой.\")\n",
    "    bad_quantity = [0,0,0]\n",
    "    total_quantity = [0, 0, 0]\n",
    "    for i in range(1, limit):\n",
    "        bad_quantity[0]+=i*train[i]\n",
    "        bad_quantity[1]+=i*test[i]\n",
    "        bad_quantity[2]+=i*val[i]\n",
    "    for key in train.keys():\n",
    "        total_quantity[0]+=key*train[key]\n",
    "        total_quantity[1]+=key*test[key]\n",
    "        total_quantity[2]+=key*val[key]\n",
    "    if limit>1:\n",
    "        plt.title(\"График для ограничения \"+ str(limit))\n",
    "        plt.suptitle(\"Для тренировочного датасета: отфильтровано \" + str(bad_quantity[0]) + \" из \" + str(total_quantity[0]) + \n",
    "                     \", т. е. \"  + \"%.2f\" % (100 * bad_quantity[0]/total_quantity[0]) + \"%.\\n\"\n",
    "                     \"Для тестового датасета: отфильтровано \" + str(bad_quantity[1]) + \" из \" + str(total_quantity[1]) + \n",
    "                     \", т. е. \"  + \"%.2f\" % (100 * bad_quantity[1]/total_quantity[1]) + \"%.\\n\"\n",
    "                     \"Для валидационного датасета: отфильтровано \" + str(bad_quantity[2]) + \" из \" + str(total_quantity[2]) + \n",
    "                     \", т. е. \"  + \"%.2f\" % (100 * bad_quantity[2]/total_quantity[2]) + \"%.\\n\"\n",
    "                    )\n",
    "    plt.savefig(\"img.png\", dpi=300)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поднимаем данные при помощи pickle, причём первые два словаря в c2v.dict это train_token и train_path, они нам не нужны. После этого загружаем всё в гистограмму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_info(path):\n",
    "    with open(path, \"rb\") as file:\n",
    "        token = pickle.load(file)\n",
    "        path = pickle.load(file)\n",
    "        target = pickle.load(file)\n",
    "    print(\"Train\", \"total:\", len(target), \"total lines\")\n",
    "    print(\"Validate\", \"total:\", len(token), \"total lines\")\n",
    "    print(\"Test\", \"total:\", len(path), \"total lines\")\n",
    "\n",
    "    historgram(target, \"train\", path, \"test\",  token, \"validate\", int(input()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train total: 40651 total lines\n",
      "Validate total: 7441 total lines\n",
      "Test total: 22878 total lines\n"
     ]
    }
   ],
   "source": [
    "dict_info(\"dataset/java-small/java-small.c2v.dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гистограмма топ N имён функций по встречаемости из всех наборов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VOCAB_PATH = \"dataset/java-small/java-small.train.functions.vocab\"\n",
    "TEST_VOCAB_PATH = \"dataset/java-small/java-small.test.functions.vocab\"\n",
    "VALIDATION_VOCAB_PATH = \"dataset/java-small/java-small.validation.functions.vocab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим все vocab-ы в pandas-таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_VOCAB_PATH, sep=' ', names=[\"Function\", \"Frequency\"])\n",
    "test_df = pd.read_csv(TEST_VOCAB_PATH, sep=' ', names=[\"Function\", \"Frequency\"])\n",
    "validation_df = pd.read_csv(VALIDATION_VOCAB_PATH, sep=' ', names=[\"Function\", \"Frequency\"])\n",
    "\n",
    "train_df[\"Dataset\"] = [\"train\"] * len(train_df)\n",
    "test_df[\"Dataset\"] = [\"test\"] * len(test_df)\n",
    "validation_df[\"Dataset\"] = [\"validation\"] * len(validation_df)\n",
    "\n",
    "display(train_df.sort_values(by=\"Function\"))\n",
    "display(test_df.sort_values(by=\"Function\"))\n",
    "display(validation_df.sort_values(by=\"Function\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгруппируем по дубликатам и просуммируем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = pd.concat([train_df, test_df, validation_df], ignore_index=True)\n",
    "conc = conc.groupby('Function').sum().reset_index()\n",
    "\n",
    "display(conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим гистограмму встречаемости слов для заданного датасета\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_histogram(data, name, limit = 50):\n",
    "    cnt = Counter()\n",
    "    freqs = data[\"Frequency\"]\n",
    "    targets = data[\"Function\"]\n",
    "    for seq, freq in zip(targets, freqs):\n",
    "        cnt.update({seq: freq})\n",
    "\n",
    "    srt = sorted(cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "    srt_labels, srt_freqs = list(map(list, zip(*srt)))\n",
    "    srt_labels = srt_labels[:limit] + ['other words']\n",
    "    srt_freqs = srt_freqs[:limit] + [sum(srt_freqs[limit:])]\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.title(f\"Распределение первых {limit} функций по встречаемости в \" + name + \" датасете\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.margins(0.025)\n",
    "    plt.bar(srt_labels, srt_freqs)\n",
    "    plt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_histogram(conc, \"объединённом\")\n",
    "target_histogram(train_df, \"тренировочном\")\n",
    "target_histogram(test_df, \"тестовом\")\n",
    "target_histogram(validation_df, \"валидационном\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
